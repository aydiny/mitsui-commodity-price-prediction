{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e199802",
   "metadata": {
    "papermill": {
     "duration": 0.003911,
     "end_time": "2025-10-06T23:50:05.074076",
     "exception": false,
     "start_time": "2025-10-06T23:50:05.070165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **MITSUI Commodity Prediction - Inference**\n",
    "This notebook performs inference using pre-trained models and artifacts.\n",
    "\n",
    "**Data Provenance 1:** The models and scaler used in this notebook are sourced from the following private Kaggle Dataset, which was manually created from the output of the training notebook.\n",
    "\n",
    "**Dataset Name 1**: models-scaler-lstm-mi-rf-allhist\n",
    "\n",
    "**Link to Training Code:** The artifacts were generated by Version 25 of the \"AAY_Mitsui_LSTM_training_MI_RF\" notebook, available here:  https://www.kaggle.com/code/aliaydnyldz/aay-mitsui-lstm-training-mi-rf/notebook?scriptVersionId=266096138\n",
    "\n",
    "*****************************\n",
    "\n",
    "**Data Provenance 2:** The feature selection dictionary used in this notebook are sourced from the following private Kaggle Dataset, which was manually created from the output of the training notebook.\n",
    "\n",
    "**Dataset Name 2:**  mi-rf-feature-selection\n",
    "\n",
    "**Link to Training Code:** The artifacts were generated by Version 12 of the \"AAY_Mitsui_LSTM_training_MI_RF\" notebook, available here:  https://www.kaggle.com/code/aliaydnyldz/aay-mitsui-lstm-training-mi-rf/notebook?scriptVersionId=265588771"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137af4e6",
   "metadata": {
    "papermill": {
     "duration": 0.00293,
     "end_time": "2025-10-06T23:50:05.080454",
     "exception": false,
     "start_time": "2025-10-06T23:50:05.077524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f898c97a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-06T23:50:05.087459Z",
     "iopub.status.busy": "2025-10-06T23:50:05.087235Z",
     "iopub.status.idle": "2025-10-06T23:50:30.562587Z",
     "shell.execute_reply": "2025-10-06T23:50:30.561724Z"
    },
    "id": "9MZYM22ZRoBH",
    "papermill": {
     "duration": 25.480591,
     "end_time": "2025-10-06T23:50:30.564078",
     "exception": false,
     "start_time": "2025-10-06T23:50:05.083487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 23:50:13.563989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759794613.979805      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759794614.098164      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Setup ------------------\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "import kaggle_evaluation.mitsui_inference_server\n",
    "import joblib\n",
    "import json\n",
    "import pickle\n",
    "import psutil\n",
    "import time\n",
    "import traceback\n",
    "import concurrent.futures\n",
    "\n",
    "# Global variables\n",
    "timesteps =5\n",
    "n_future=4\n",
    "emw_lookback=26\n",
    "prediction_call_counter = 0\n",
    "\n",
    "# ------------------ Data Loading ------------------\n",
    "train_df = pl.read_csv('/kaggle/input/mitsui-commodity-prediction-challenge/train.csv').to_pandas()\n",
    "train_labels_df = pl.read_csv('/kaggle/input/mitsui-commodity-prediction-challenge/train_labels.csv').to_pandas()\n",
    "target_pairs_df = pl.read_csv('/kaggle/input/mitsui-commodity-prediction-challenge/target_pairs.csv').to_pandas()\n",
    "test_df = pl.read_csv('/kaggle/input/mitsui-commodity-prediction-challenge/test.csv').to_pandas()\n",
    "\n",
    "# Remove last 90 rows from training data\n",
    "train_df_test = train_df[1870:]\n",
    "train_df= train_df[:1870]\n",
    "\n",
    "train_labels_train = train_labels_df[:1870]\n",
    "train_labels_test = train_labels_df[1870:]\n",
    "\n",
    "# 2. Global constants for paths\n",
    "FEATURES_PATH = '/kaggle/input/mi-rf-feature-selection'\n",
    "MODEL_SCALER_DATASET_PATH = '/kaggle/input/models-scaler-lstm-mi-rf-allhist'\n",
    "\n",
    "\n",
    "# --- Initialize Global Caches and History ---\n",
    "# These will persist across all calls to the predict() function\n",
    "models_cache = {}\n",
    "scalers_cache = {}\n",
    "feature_dict_cache = {}\n",
    "history_df = None \n",
    "all_features_cache = None\n",
    "scaled_features_cache = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d501ad",
   "metadata": {
    "papermill": {
     "duration": 0.003226,
     "end_time": "2025-10-06T23:50:30.582473",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.579247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7aad61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T23:50:30.590772Z",
     "iopub.status.busy": "2025-10-06T23:50:30.590215Z",
     "iopub.status.idle": "2025-10-06T23:50:30.649418Z",
     "shell.execute_reply": "2025-10-06T23:50:30.648849Z"
    },
    "papermill": {
     "duration": 0.064659,
     "end_time": "2025-10-06T23:50:30.650672",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.586013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop \"US_Stock_GOLD* columns as too few data points (and nothing to test)\"\n",
    "train_df.drop(columns=[\n",
    "'US_Stock_GOLD_adj_open',\n",
    "'US_Stock_GOLD_adj_high',\n",
    "'US_Stock_GOLD_adj_low',\n",
    "'US_Stock_GOLD_adj_close',\n",
    "'US_Stock_GOLD_adj_volume'], inplace=True)\n",
    "\n",
    "#fill the gaps on the time series with the most recent data point available, then the soonest\n",
    "train_df.fillna(method='ffill', inplace=True)\n",
    "train_df.fillna(method='bfill', inplace=True)\n",
    "train_df.fillna(0, inplace=True)\n",
    "\n",
    "test_df.fillna(method='ffill', inplace=True)\n",
    "test_df.fillna(method='bfill', inplace=True)\n",
    "train_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed0022",
   "metadata": {
    "papermill": {
     "duration": 0.003114,
     "end_time": "2025-10-06T23:50:30.657136",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.654022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Log Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cec627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T23:50:30.664163Z",
     "iopub.status.busy": "2025-10-06T23:50:30.663939Z",
     "iopub.status.idle": "2025-10-06T23:50:30.667981Z",
     "shell.execute_reply": "2025-10-06T23:50:30.667267Z"
    },
    "papermill": {
     "duration": 0.008771,
     "end_time": "2025-10-06T23:50:30.669038",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.660267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_memory_usage(stage_name=\"\"):\n",
    "    \"\"\"\n",
    "    Logs the current memory usage of the process to the console.\n",
    "    \"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_gb = process.memory_info().rss / (1024 ** 3)  # Resident Set Size in GB\n",
    "    print(f\"--- RAM USAGE at '{stage_name}': {memory_gb:.3f} GB ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa97b20",
   "metadata": {
    "papermill": {
     "duration": 0.002875,
     "end_time": "2025-10-06T23:50:30.674903",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.672028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Return Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e642393b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-06T23:50:30.681499Z",
     "iopub.status.busy": "2025-10-06T23:50:30.681262Z",
     "iopub.status.idle": "2025-10-06T23:50:30.687517Z",
     "shell.execute_reply": "2025-10-06T23:50:30.687051Z"
    },
    "id": "9MZYM22ZRoBH",
    "papermill": {
     "duration": 0.010858,
     "end_time": "2025-10-06T23:50:30.688661",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.677803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------ Utility Functions ------------------\n",
    "def generate_log_returns_forward(data, lag):\n",
    "    log_returns = pd.Series(np.nan, index=data.index)\n",
    "    for t in range(len(data)-lag):\n",
    "        try:\n",
    "            log_returns.iloc[t] = np.log(data.iloc[t + 1 + lag ] / data.iloc[t + 1])\n",
    "        except Exception:\n",
    "            log_returns.iloc[t] = np.nan\n",
    "    return log_returns\n",
    "\n",
    "\n",
    "def generate_targets(column_a: pd.Series, lag: int, column_b: pd.Series = None) -> pd.Series:\n",
    "    a_returns = generate_log_returns_forward(column_a, lag)\n",
    "    if column_b is not None:\n",
    "        b_returns = generate_log_returns_forward(column_b, lag)\n",
    "        return a_returns - b_returns\n",
    "    else:\n",
    "        return a_returns\n",
    "\n",
    "def generate_log_returns_backward(data, lag):\n",
    "    log_returns = pd.Series(np.nan, index=data.index)\n",
    "    for t in range(lag,len(data)):\n",
    "        try:\n",
    "            log_returns.iloc[t] = np.log(data.iloc[t] / data.iloc[t-lag])\n",
    "        except Exception:\n",
    "            log_returns.iloc[t] = np.nan\n",
    "    return log_returns\n",
    "\n",
    "def generate_differences_backward(data, lag):\n",
    "    differences = pd.Series(np.nan, index=data.index)\n",
    "    for t in range(lag,len(data)):\n",
    "        try:\n",
    "            differences.iloc[t] = data.iloc[t] - data.iloc[t-lag]\n",
    "        except Exception:\n",
    "            differences.iloc[t] = np.nan\n",
    "    return differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2e8ec",
   "metadata": {
    "id": "opxkIid1BFd0",
    "papermill": {
     "duration": 0.003549,
     "end_time": "2025-10-06T23:50:30.695126",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.691577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc854c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T23:50:30.701793Z",
     "iopub.status.busy": "2025-10-06T23:50:30.701595Z",
     "iopub.status.idle": "2025-10-06T23:50:30.705384Z",
     "shell.execute_reply": "2025-10-06T23:50:30.704559Z"
    },
    "id": "rHFRwrFHBFd1",
    "papermill": {
     "duration": 0.008272,
     "end_time": "2025-10-06T23:50:30.706372",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.698100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26e123",
   "metadata": {
    "id": "W8sgrm577GP_",
    "papermill": {
     "duration": 0.002809,
     "end_time": "2025-10-06T23:50:30.712177",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.709368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc0149e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T23:50:30.719134Z",
     "iopub.status.busy": "2025-10-06T23:50:30.718936Z",
     "iopub.status.idle": "2025-10-06T23:50:30.739146Z",
     "shell.execute_reply": "2025-10-06T23:50:30.738608Z"
    },
    "id": "RyHk3HWU7DFP",
    "papermill": {
     "duration": 0.025019,
     "end_time": "2025-10-06T23:50:30.740095",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.715076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_features_returns (feature_df:pd.DataFrame)->pd.DataFrame:\n",
    "\n",
    "    log_memory_usage(\"Before feature engineering\")\n",
    "\n",
    "    # First, downcast the input DataFrame to save memory from the start\n",
    "    for col in feature_df.select_dtypes(include=['float64']).columns:\n",
    "        feature_df[col] = feature_df[col].astype('float32')\n",
    "    for col in feature_df.select_dtypes(include=['int64']).columns:\n",
    "        feature_df[col] = feature_df[col].astype('int32')\n",
    "\n",
    "    print(\"generating feature returns:\")\n",
    "    # ... code for feature returns ...\n",
    "    \n",
    "    print(\"generating feature returns: \\n\\n\")\n",
    "    \n",
    "    if 'date_id' in feature_df.columns:\n",
    "        feature_df.drop(columns=['date_id'], inplace=True)\n",
    "\n",
    "    if 'is_scored' in feature_df.columns:\n",
    "        feature_df.drop(columns=['is_scored'], inplace=True)\n",
    "    \n",
    "    return_feature_df = pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "    fx_return_feature_df = pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "    volume_feature_df = pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "    oi_feature_df = pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "    open_price_df = pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "    close_price_df = pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "    low_price_df = pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "    high_price_df = pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "    volume_df = pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "    fx_rate_df= pd.DataFrame(index=feature_df.index).astype('float32')\n",
    "\n",
    "    #calculate 1-day log returns for different lags, fillna\n",
    "    for col  in tqdm(feature_df, total=len(feature_df.columns)):\n",
    "        if ('volume' in col) | ('Volume' in col):\n",
    "            volume_feature_df[col] = feature_df[col]\n",
    "            volume_feature_df[col].fillna(method='ffill', inplace=True)\n",
    "            volume_feature_df[col].fillna(method='bfill', inplace=True)\n",
    "\n",
    "            if 'volume' in col:\n",
    "                colNew= col.replace('_volume', '')\n",
    "            elif 'Volume' in col:\n",
    "                colNew= col.replace('_Volume', '')\n",
    "\n",
    "            volume_df[colNew] = feature_df[col]\n",
    "            volume_df[colNew].fillna(method='ffill', inplace=True)\n",
    "            volume_df[colNew].fillna(method='bfill', inplace=True)\n",
    "\n",
    "        elif 'open_interest' in col:\n",
    "            oi_feature_df[col]=generate_differences_backward(feature_df[col], 1)\n",
    "            oi_feature_df[col].fillna(method='ffill', inplace=True)\n",
    "            oi_feature_df[col].fillna(method='bfill', inplace=True)\n",
    "\n",
    "        elif ('close' in col) | ('Close' in col): #price data\n",
    "\n",
    "            return_feature_df[col] = generate_log_returns_backward(feature_df[col], 1)\n",
    "            return_feature_df[col].fillna(0, inplace=True)\n",
    "\n",
    "            if 'close'in col:\n",
    "              colNew= col.replace('_close', '')\n",
    "            elif 'Close'in col:\n",
    "              colNew= col.replace('_Close', '')\n",
    "\n",
    "            close_price_df[colNew] = feature_df[col]\n",
    "            close_price_df[colNew].fillna(method='ffill', inplace=True)\n",
    "            close_price_df[colNew].fillna(method='bfill', inplace=True)\n",
    "\n",
    "        elif 'FX_' in col: #FX rate data\n",
    "\n",
    "            fx_return_feature_df[col] = generate_log_returns_backward(feature_df[col], 1)\n",
    "            fx_return_feature_df[col].fillna(0, inplace=True)\n",
    "\n",
    "            fx_rate_df[col] = feature_df[col]\n",
    "            fx_rate_df[col].fillna(method='ffill', inplace=True)\n",
    "            fx_rate_df[col].fillna(method='bfill', inplace=True)\n",
    "\n",
    "        elif ('open'in col) | ('Open'in col):\n",
    "\n",
    "            if 'open'in col:\n",
    "              colNew= col.replace('_open', '')\n",
    "            elif 'Open'in col:\n",
    "              colNew= col.replace('_Open', '')\n",
    "\n",
    "            open_price_df[colNew] = feature_df[col]\n",
    "            open_price_df[colNew].fillna(method='ffill', inplace=True)\n",
    "            open_price_df[colNew].fillna(method='bfill', inplace=True)\n",
    "\n",
    "        elif ('low'in col) | ('Low'in col):\n",
    "\n",
    "            if 'low'in col:\n",
    "              colNew= col.replace('_low', '')\n",
    "            elif 'Low'in col:\n",
    "              colNew= col.replace('_Low', '')\n",
    "\n",
    "            low_price_df[colNew] = feature_df[col]\n",
    "            low_price_df[colNew].fillna(method='ffill', inplace=True)\n",
    "            low_price_df[colNew].fillna(method='bfill', inplace=True)\n",
    "\n",
    "        elif ('high'in col) | ('High'in col):\n",
    "\n",
    "            if 'high'in col:\n",
    "              colNew= col.replace('_high', '')\n",
    "            elif 'High'in col:\n",
    "              colNew= col.replace('_High', '')\n",
    "\n",
    "            high_price_df[colNew] = feature_df[col]\n",
    "            high_price_df[colNew].fillna(method='ffill', inplace=True)\n",
    "            high_price_df[colNew].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # ... code for price structure ...\n",
    "    log_memory_usage(\"Before price structure features\") # <<< KEY ADDITION\n",
    "    \n",
    "    print (\"generating price structure and volatility features\", \"\\n\")\n",
    "    \n",
    "    dailyPriceRange= (high_price_df - low_price_df).astype('float32')\n",
    "    dailyPriceRange.columns = [col + '_dailyPriceRange' for col in dailyPriceRange.columns]\n",
    "\n",
    "    intradayReturn= (open_price_df - close_price_df).astype('float32')\n",
    "    intradayReturn.columns = [col + '_intradayReturn' for col in intradayReturn.columns]\n",
    "\n",
    "    positionInRange= ((close_price_df - low_price_df) / (high_price_df - low_price_df)).astype('float32')\n",
    "    positionInRange.columns = [col + '_positionInRange' for col in positionInRange.columns]\n",
    "\n",
    "    bodyToShadowRatio= (close_price_df - open_price_df) / (high_price_df - low_price_df).astype('float32')\n",
    "    bodyToShadowRatio.columns = [col + '_bodyToShadowRatio' for col in bodyToShadowRatio.columns]\n",
    "\n",
    "    log_memory_usage(\"Before creating momentum indicators\") \n",
    "    \n",
    "    print (\"creating momentum indicators\", '\\n')\n",
    "\n",
    "    momentumIndicators = create_technical_indicator_features(close_price_df, volume_df).astype('float32')\n",
    "    fx_momentumIndicators = create_technical_indicator_features(fx_rate_df).astype('float32')\n",
    "\n",
    "    log_memory_usage(\"After momentum indicators\") \n",
    "    \n",
    "    print (\"momentum indicators completed\", '\\n')\n",
    "    \n",
    "   # Start with the first DataFrame as the base\n",
    "    all_feature_df = return_feature_df.copy()\n",
    "    del return_feature_df\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After adding return_feature_df\")\n",
    "    \n",
    "    # Concatenate and delete one by one\n",
    "    all_feature_df = pd.concat([all_feature_df, fx_return_feature_df], axis=1)\n",
    "    del fx_return_feature_df\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After adding fx_return_feature_df\")\n",
    "    \n",
    "    all_feature_df = pd.concat([all_feature_df, volume_feature_df], axis=1)\n",
    "    del volume_feature_df\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After adding volume_feature_df\")\n",
    "    \n",
    "    all_feature_df = pd.concat([all_feature_df, oi_feature_df], axis=1)\n",
    "    del oi_feature_df\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After adding oi_feature_df\")\n",
    "    \n",
    "    all_feature_df = pd.concat([all_feature_df, dailyPriceRange], axis=1)\n",
    "    del dailyPriceRange\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After adding dailyPriceRange\")\n",
    "    \n",
    "    all_feature_df = pd.concat([all_feature_df, intradayReturn], axis=1)\n",
    "    del intradayReturn\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After adding intradayReturn\")\n",
    "    \n",
    "    all_feature_df = pd.concat([all_feature_df, positionInRange], axis=1)\n",
    "    del positionInRange\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After adding positionInRange\")\n",
    "    \n",
    "    all_feature_df = pd.concat([all_feature_df, bodyToShadowRatio], axis=1)\n",
    "    del bodyToShadowRatio\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After adding bodyToShadowRatio\")\n",
    "    \n",
    "    all_feature_df = pd.concat([all_feature_df, momentumIndicators], axis=1)\n",
    "    del momentumIndicators\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After adding momentumIndicators\")\n",
    "    \n",
    "    all_feature_df = pd.concat([all_feature_df, fx_momentumIndicators], axis=1)\n",
    "    del fx_momentumIndicators\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"After final concatenation\")\n",
    "    \n",
    "    print(\"All features combined successfully.\")\n",
    "\n",
    "    \n",
    "    for col in all_feature_df.columns:\n",
    "        if all_feature_df[col].dtype == 'float64':\n",
    "            all_feature_df[col] = all_feature_df[col].astype('float32')\n",
    "\n",
    "    all_feature_df.fillna(method='ffill', inplace=True)\n",
    "    all_feature_df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    #remove columns which have all NaN values\n",
    "    all_feature_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    return all_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643877e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T23:50:30.747039Z",
     "iopub.status.busy": "2025-10-06T23:50:30.746821Z",
     "iopub.status.idle": "2025-10-06T23:50:30.753465Z",
     "shell.execute_reply": "2025-10-06T23:50:30.752919Z"
    },
    "id": "9gXMaIAkBFd6",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.011372,
     "end_time": "2025-10-06T23:50:30.754417",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.743045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_technical_indicator_features(close_price_df, volume_df=None):\n",
    "    \"\"\"\n",
    "    Engineers momentum features from a DataFrame with 'Close' and 'Volume'.\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Check if there is volume time series corresponding to the priceSeries\n",
    "\n",
    "    1. replace \"_close\" or \"_Close\" at the end of the priceSeries name with \"_volume\" , call it volumeName\n",
    "\n",
    "    2. check if volumeName existis in scaledFeatures columns\n",
    "    \"\"\"\n",
    "\n",
    "    df_macd=pd.DataFrame(index=close_price_df.index)\n",
    "    df_rsi=pd.DataFrame(index=close_price_df.index)\n",
    "    df_momentum_x_volume=pd.DataFrame(index=close_price_df.index)\n",
    "\n",
    "    #for priceSerie in priceSeries:\n",
    "    # 1. Price Rate of Change (10-day and 21-day momentum)\n",
    "    momentum_10d = close_price_df.pct_change(periods=10).astype('float32')\n",
    "    momentum_21d = close_price_df.pct_change(periods=21).astype('float32')\n",
    "\n",
    "    # 2. MACD\n",
    "    ema_12 = close_price_df.ewm(span=12, adjust=False).mean().astype('float32')\n",
    "    ema_26 = close_price_df.ewm(span=emw_lookback, adjust=False).mean().astype('float32')\n",
    "    macd= ema_12 - ema_26\n",
    "    df_macd = macd.ewm(span=9, adjust=False).mean().astype('float32')\n",
    "    df_macd.columns = [col + '_macd' for col in df_macd.columns]\n",
    "\n",
    "    # 3. Relative Strength Index (RSI)\n",
    "    delta = close_price_df.diff().astype('float32')\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean().astype('float32')\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean().astype('float32')\n",
    "    rs = gain / loss\n",
    "    df_rsi= 100 - (100 / (1 + rs))\n",
    "    df_rsi.columns = [col + '_rsi' for col in df_rsi.columns]\n",
    "\n",
    "\n",
    "    # 4. Volume-Adjusted Momentum\n",
    "    if volume_df is not None:\n",
    "      volume_ma_20d = volume_df.rolling(window=20).mean().astype('float32')\n",
    "      df_momentum_x_volume = momentum_10d * (volume_df / volume_ma_20d)\n",
    "      df_momentum_x_volume.columns = [col + '_momentum_x_volume' for col in df_momentum_x_volume.columns]\n",
    "\n",
    "    df_techIndicators=pd.concat([df_macd, df_rsi, df_momentum_x_volume], axis=1)\n",
    "\n",
    "    return df_techIndicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fee408",
   "metadata": {
    "papermill": {
     "duration": 0.002728,
     "end_time": "2025-10-06T23:50:30.760159",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.757431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Multistep Data Set for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a187057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T23:50:30.766941Z",
     "iopub.status.busy": "2025-10-06T23:50:30.766734Z",
     "iopub.status.idle": "2025-10-06T23:50:30.772775Z",
     "shell.execute_reply": "2025-10-06T23:50:30.772252Z"
    },
    "id": "HOIknWoPu8A1",
    "papermill": {
     "duration": 0.010722,
     "end_time": "2025-10-06T23:50:30.773773",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.763051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_multi_step_dataset(feature_series,timesteps, n_future ,target_series=None) -> tuple[np.ndarray, np.ndarray] | np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a dataset for multi-step forecasting.\n",
    "\n",
    "    Args:\n",
    "        feature_series: Series of input features.\n",
    "        target_series: Series of target values, or None.\n",
    "        timesteps: The lookback window size.\n",
    "        n_future: The number of future steps to predict.\n",
    "    \"\"\"\n",
    "    print(\"generating multi-step dataset: \\n\\n\")\n",
    "    print(\"shape of the input features_series: \", feature_series.shape)\n",
    "\n",
    "    if target_series is not None:\n",
    "        target_name = target_series.name\n",
    "         #find the 'lag' column for the row where 'target_name' == target_name on the target_pairs dataframe\n",
    "        lag = target_pairs_df[target_pairs_df['target'] == target_name]['lag'].iloc[0]\n",
    "         #find the 'pair' column for the row where 'target_name' == target_name on the target_pairs dataframe\n",
    "        pair = target_pairs_df[target_pairs_df['target'] == target_name]['pair'].iloc[0]\n",
    "\n",
    "        if ' - ' in pair:\n",
    "          a, b = pair.split(' - ')\n",
    "          target_series_replicated = generate_targets(train_df[a],1,train_df[b])\n",
    "\n",
    "        else:\n",
    "          target_series_replicated=generate_targets(train_df[pair],1)\n",
    "\n",
    "        target_series_replicated.fillna(0, inplace=True)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "\n",
    "    if target_series is None: # this is only for inference / prediction\n",
    "        for i in range(len(feature_series)-1, len(feature_series)):\n",
    "            # Input sequence (past data)\n",
    "            X.append(feature_series[i-timesteps:i])\n",
    "        X_array = np.array(X)\n",
    "        return X_array\n",
    "    else:\n",
    "        # Adjust the loop to ensure there's enough data for the future sequence\n",
    "        for i in range(timesteps, len(feature_series) - n_future):\n",
    "            # Input sequence (past data)\n",
    "            X.append(feature_series[i-timesteps:i])    \n",
    "            # Output sequence (future data)\n",
    "            y.append(target_series_replicated[i-timesteps : i-timesteps+n_future].to_numpy())\n",
    "        X_array = np.array(X)\n",
    "        y_array = np.array(y)\n",
    "        return X_array, y_array\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4840abc",
   "metadata": {
    "papermill": {
     "duration": 0.002768,
     "end_time": "2025-10-06T23:50:30.779529",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.776761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c32c52b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T23:50:30.786438Z",
     "iopub.status.busy": "2025-10-06T23:50:30.786219Z",
     "iopub.status.idle": "2025-10-06T23:50:31.001706Z",
     "shell.execute_reply": "2025-10-06T23:50:31.000877Z"
    },
    "papermill": {
     "duration": 0.220665,
     "end_time": "2025-10-06T23:50:31.003071",
     "exception": false,
     "start_time": "2025-10-06T23:50:30.782406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ENVIRONMENT VARIABLE CHECK ---\n",
      "Value of KAGGLE_IS_COMPETITION_RERUN: None\n",
      "Value of KAGGLE_KERNEL_RUN_TYPE:    Batch\n",
      "------------------------------------\n",
      ">>> EXECUTING COMMIT RUN ('Batch'). Creating dummy file.\n"
     ]
    }
   ],
   "source": [
    "def predict(test: pl.DataFrame,\n",
    "    label_lags_1_batch: pl.DataFrame,\n",
    "    label_lags_2_batch: pl.DataFrame,\n",
    "    label_lags_3_batch: pl.DataFrame,\n",
    "    label_lags_4_batch: pl.DataFrame,\n",
    "    ) -> pl.DataFrame | pd.DataFrame:\n",
    "    \n",
    "    \n",
    "    # --- Capture original structure ---\n",
    "    original_index = test.to_pandas().index\n",
    "    current_date_id = test.to_pandas()['date_id'].iloc[0]\n",
    "    # ----------------------------------\n",
    "\n",
    "    test_df_new = test.to_pandas()\n",
    "    \n",
    "    print ('test_df shape from server is:', test.shape, '\\n' )\n",
    "    print ('test df head:', test.head(), '\\n\\n' )\n",
    "    print ('test df tail:', test.tail(), '\\n\\n' )\n",
    "    print ('label_lags_1_batch shape:', label_lags_1_batch.shape, '\\n\\n' )\n",
    "    print ('label_lags_2_batch shape:', label_lags_2_batch.shape, '\\n\\n' )\n",
    "    print ('label_lags_3_batch shape:', label_lags_3_batch.shape, '\\n\\n' )\n",
    "    print ('label_lags_4_batch shape:', label_lags_4_batch.shape, '\\n\\n' )\n",
    "    \n",
    "    # --- Tell the function we intend to modify the global variables ---\n",
    "    global models_cache, scalers_cache, feature_dict_cache, history_df, all_features_cache, scaled_features_cache, prediction_call_counter\n",
    "\n",
    "    prediction_call_counter+=1\n",
    "    # Get the date_id from the incoming data\n",
    "    current_date_id = test.to_pandas()['date_id'].iloc[0]\n",
    "    \n",
    "    print(f\"--- Prediction Call #{prediction_call_counter} | Processing Date ID: {current_date_id} ---\")\n",
    "    \n",
    "    try:\n",
    "        # --- 1. LOAD STATIC ARTIFACTS (on first run only) ---\n",
    "        if not feature_dict_cache:\n",
    "            with open(f'{FEATURES_PATH}/featureSetDict_MI_RF.json', 'r') as f: feature_dict_cache['main'] = json.load(f)\n",
    "        if not scalers_cache:\n",
    "            with open(f'{MODEL_SCALER_DATASET_PATH}/scaler_fit_train.pkl', 'rb') as f: scalers_cache['main'] = pickle.load(f)\n",
    "        \n",
    "        featureSetDict = feature_dict_cache['main']\n",
    "        scaler_train = scalers_cache['main']\n",
    "        \n",
    "        if not models_cache:\n",
    "            # 3. Eagerly load all 424 models into the global cache\n",
    "            models_dir = os.path.join(MODEL_SCALER_DATASET_PATH, 'models')\n",
    "            print(f\"Eagerly loading all models from: {models_dir}\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if os.path.exists(models_dir):\n",
    "                for file_name in os.listdir(models_dir):\n",
    "                    if file_name.endswith('_model.keras'):\n",
    "                        target_name = file_name.replace('_model.keras', '')\n",
    "                        model_path = os.path.join(models_dir, file_name)\n",
    "                        models_cache[target_name] = load_model(model_path)\n",
    "                        end_time = time.time()\n",
    "                    else:\n",
    "                        raise FileNotFoundError(f\"FATAL ERROR: Models directory not found at {models_dir}\")\n",
    "                print(f\"Successfully loaded {len(models_cache)} models in {end_time - start_time:.2f} seconds.\") \n",
    "        \n",
    "        if history_df is None:\n",
    "            print ('no time series history, attempt to generate one ')\n",
    "            history_df = train_df[train_df['date_id']<current_date_id] \n",
    "            print ('size of history_df before current prediction date: ',history_df.shape)\n",
    "            if 'is_scored' not in history_df.columns:\n",
    "                print(\"Adding 'is_scored' column to train_df.\")\n",
    "                history_df['is_scored'] = False\n",
    "        \n",
    "        # Subsequent runs, append new data and maintain history length\n",
    "        history_df = pd.concat([history_df, test_df_new], ignore_index=True)\n",
    "        print ('size of history_df before truncating: ',history_df.shape)\n",
    "        \n",
    "        # Ensure we don't keep more history than needed\n",
    "        history_df = history_df.iloc[-(emw_lookback + 5):,:] # Keep a small buffer\n",
    "        print ('emw_lookback: ',emw_lookback)\n",
    "        print ('size of history_df after truncating: ',history_df.shape)\n",
    "        \n",
    "        # Now, create the DataFrame for this prediction run\n",
    "        test_df_all = history_df.copy()\n",
    "        test_df_all.fillna(method='ffill', inplace=True)\n",
    "        test_df_all.fillna(method='bfill', inplace=True)\n",
    "        test_df_all.fillna(0, inplace=True)\n",
    "        \n",
    "         # #if any of test_df_all.columns begins with \"US_Stock_GOLD_adj_\", remove that column\n",
    "        for col in test_df_all.columns:\n",
    "            if col.startswith(\"US_Stock_GOLD_adj_\"):\n",
    "                test_df_all.drop(columns=[col], inplace=True)\n",
    "\n",
    "        print ('test_df_all head():\\n',test_df_all.head(), '\\n' )\n",
    "        print ('is scored column:\\n ', test_df_all['is_scored'])\n",
    "\n",
    "        last_is_scored = test_df_all['is_scored'].iloc[-1]\n",
    "        \n",
    "        if not last_is_scored:\n",
    "            # If the current date is not scored, we don't need to predict.\n",
    "            # Return a default DataFrame of zeros immediately.\n",
    "            print(f\"SKIP Predictions: The date is not scored.\")\n",
    "            target_columns = target_pairs_df['target'].tolist()\n",
    "            return pl.DataFrame({col: 0.0 for col in target_columns})\n",
    "        \n",
    "        if scaled_features_cache  is None:\n",
    "            allFeatureReturns_test= generate_features_returns(test_df_all)\n",
    "            allFeatureReturns_test_cut=allFeatureReturns_test[-(timesteps+1):] # need the last \"timestep\" of observations for multi-data set\n",
    "            scaledFeatures_test = scaler_train.transform(allFeatureReturns_test_cut)\n",
    "            scaledFeatures_test_df = pd.DataFrame(scaledFeatures_test, columns=allFeatureReturns_test.columns)\n",
    "            print('shape of scaledFeatures_test_df :/n/n')\n",
    "            print(scaledFeatures_test_df.shape)\n",
    "            # Store in the global cache for the next run\n",
    "            scaled_features_cache = scaledFeatures_test_df.copy()            \n",
    "        else:\n",
    "            print(\"Using cached features.\")\n",
    "            scaledFeatures_test_df = scaled_features_cache\n",
    "            \n",
    "        \n",
    "        t_it= time.time()\n",
    "        def predict_for_single_target(target_info, features_df, feature_map):\n",
    "            \"\"\"Processes a single target.\"\"\"\n",
    "            target_name, lag, pair = target_info\n",
    "            # 1. Prepare data for this specific target\n",
    "            scaledFeatureSubset_test = features_df[feature_map[target_name]]\n",
    "            print('shape of scaledFeatureSubset_test into multi-step:/n/n')\n",
    "            print(scaledFeatureSubset_test.shape)\n",
    "            X_lstm = create_multi_step_dataset(scaledFeatureSubset_test, timesteps=timesteps, n_future=n_future)\n",
    "            print ('predictor shape: ',X_lstm.shape)\n",
    "            model = models_cache[target_name]\n",
    "            # 3. Predict\n",
    "            pred_returns = model.predict(X_lstm)\n",
    "            print ('prediction shape: ',pred_returns.shape)\n",
    "            # 4. Aggregate\n",
    "            pred_return = 0\n",
    "            for j in range(0, lag):\n",
    "                pred_return += pred_returns[-1, j]\n",
    "            return target_name, pred_return\n",
    "        \n",
    "        # --- Parallel Execution ---\n",
    "        predictions = {}\n",
    "        # Get a list of tuples, one for each target\n",
    "        target_info_list = [(row['target'], row['lag'], row['pair']) for i, row in target_pairs_df.iterrows()]\n",
    "            \n",
    "        # Use a thread pool to execute predictions in parallel (4 workers for 4 CPU cores)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            # map applies the function to each item in the list and returns the results\n",
    "            future_to_target = {executor.submit(predict_for_single_target, info, scaledFeatures_test_df, featureSetDict):\n",
    "                                info for info in target_info_list}\n",
    "            for future in concurrent.futures.as_completed(future_to_target):\n",
    "                try:\n",
    "                    target_name, pred_return = future.result()\n",
    "                    predictions[target_name] = pred_return\n",
    "                except Exception as exc:\n",
    "                    failed_target_info = future_to_target[future]\n",
    "                    print(f\"Target '{failed_target_info[0]}' generated an exception: {exc}\")\n",
    "                    predictions[failed_target_info[0]] = 0.0\n",
    "        # --- End of Parallel Execution ---\n",
    "\n",
    "         # --- FINAL RECONSTRUCTION: COLUMNS AND INDEX ---\n",
    "        \n",
    "        print(\"Step 7.0: Constructing final ordered prediction DataFrame...\")\n",
    "    \n",
    "        sorted_target_names = sorted(predictions.keys(), key=lambda name: int(name.split('_')[1]))\n",
    "        \n",
    "        final_predictions_df = pd.DataFrame(\n",
    "            {name: [predictions[name]] for name in sorted_target_names}\n",
    "        ).astype('float64')\n",
    "        \n",
    "        final_predictions_df.index = [int(current_date_id)]\n",
    "        final_predictions_df.index.name = 'date_id'\n",
    "\n",
    "        # --- END OF RECONSTRUCTION ---\n",
    "\n",
    "        print(\"Step 7.1: Final DataFrame reconstruction complete.\")\n",
    "        print(f\"--> Final DF shape: {final_predictions_df.shape}\")\n",
    "        print(f\"--> Final DF columns: {final_predictions_df.columns.tolist()}\")\n",
    "        print(f\"--> Final DF index: {final_predictions_df.index}\")\n",
    "\n",
    "        assert isinstance(final_predictions_df, (pd.DataFrame, pl.DataFrame))\n",
    "        assert len(final_predictions_df) == 1\n",
    "        print(\">>> Prediction function finished successfully.\")\n",
    "        t_it_end= time.time()\n",
    "        print ('TOTAL TIME TAKEN for predictions' , t_it_end-t_it)\n",
    "        return final_predictions_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        # --- THE ULTIMATE SAFETY NET ---\n",
    "        print(\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"!!! A FATAL ERROR WAS CAUGHT BY THE SAFETY NET !!!\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "        traceback.print_exc() # This will print the full error to the logs.\n",
    "        \n",
    "        # Now, create and return a perfectly formatted default DataFrame.\n",
    "        # This prevents the notebook from crashing.\n",
    "        print(\">>> Returning a default DataFrame to prevent a hard crash.\")\n",
    "        \n",
    "        target_columns = target_pairs_df['target'].tolist()\n",
    "        default_df = pd.DataFrame({col: [0.0] for col in target_columns}).astype('float64')\n",
    "        \n",
    "        # Try to set the index correctly, but have a fallback.\n",
    "        try:\n",
    "            current_date_id_for_error = test.to_pandas()['date_id'].iloc[0]\n",
    "            default_df.index = [int(current_date_id_for_error)]\n",
    "            default_df.index.name = 'date_id'\n",
    "        except Exception:\n",
    "            print(\">>> Could not get date_id for error return. Using default index.\")\n",
    "\n",
    "        return default_df\n",
    "        \n",
    "server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "\n",
    "# --- ADD THIS DEBUGGING BLOCK ---\n",
    "print(\"--- ENVIRONMENT VARIABLE CHECK ---\")\n",
    "print(f\"Value of KAGGLE_IS_COMPETITION_RERUN: {os.getenv('KAGGLE_IS_COMPETITION_RERUN')}\")\n",
    "print(f\"Value of KAGGLE_KERNEL_RUN_TYPE:    {os.getenv('KAGGLE_KERNEL_RUN_TYPE')}\")\n",
    "print(\"------------------------------------\")\n",
    "# ------------------------------------\n",
    "\n",
    "# Initialize your server object as before\n",
    "server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "\n",
    "# Check if this is the final submission run (highest priority)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN') == 'true':\n",
    "    print(\">>> EXECUTING FINAL SUBMISSION RUN (server.serve())\")\n",
    "    server.serve()\n",
    "\n",
    "# If not a final submission, check if we are in an interactive session\n",
    "elif os.getenv('KAGGLE_KERNEL_RUN_TYPE') == 'Interactive':\n",
    "    print(\">>> EXECUTING INTERACTIVE RUN (server.run_local_gateway())\")\n",
    "    server.run_local_gateway(('/kaggle/input/mitsui-commodity-prediction-challenge',))\n",
    "\n",
    "# If it's neither of the above, it's a 'Batch' run (a \"Save & Commit\")\n",
    "else:\n",
    "    print(\">>> EXECUTING COMMIT RUN ('Batch'). Creating dummy file.\")\n",
    "    target_columns = target_pairs_df['target'].tolist()\n",
    "    dummy_df = pd.DataFrame({col: [0.0] for col in target_columns})\n",
    "    dummy_df.to_parquet('submission.parquet', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13613251,
     "sourceId": 94771,
     "sourceType": "competition"
    },
    {
     "datasetId": 8405619,
     "sourceId": 13264383,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8416940,
     "sourceId": 13281127,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35.14268,
   "end_time": "2025-10-06T23:50:34.213403",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-06T23:49:59.070723",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
